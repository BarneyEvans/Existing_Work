\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{subfig}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{booktabs}
\usepackage{pxfonts}
\usepackage{listings}
\usepackage{epstopdf}
\usepackage{import}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Define standard verbatim listing style
\lstdefinestyle{standard}{
  basicstyle=\fontsize{9}{9}\ttfamily,
  columns=flexible,
  breaklines=true,
  escapechar=\#
}

\newcommand\inputpgf[2]{{
\let\pgfimageWithoutPath\pgfimage
\renewcommand{\pgfimage}[2][]{\pgfimageWithoutPath[##1]{#1/##2}}
\input{#1/#2}
}}

\usepackage{tikz}
\usepackage{pgfplots}

\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}


\begin{document}
\title{Biometrics: Body Shape Recognition\\
}

\author{\IEEEauthorblockN{\textbf{David Jones}}
\IEEEauthorblockA{\textit{School of Electronics and Computer Science} \\
\textit{University of Southampton}\\
Southampton, United Kingdom \\
dsj1n15@ecs.soton.ac.uk}
}
\maketitle

\begin{abstract}
  The role of a biometric system is to identify subjects for purposes including access control and authentication. This report studies the use of body-shape features as a means to classify subjects. The approach taken utilises state-of-the-art neural networks for pose estimation and semantic segmentation before applying more traditional techniques to define feature vectors. The approach has a CCR of 91\% and EER of 33\%. Increasing performance was largely handicapped by the irregularities in the small dataset and lack of subject repetition.
\end{abstract}

\begin{IEEEkeywords}
Identification, Body Shape, Limb, Shape Descriptors, Pose Estimation, Semantic Segmentation
\end{IEEEkeywords}

\section{Introduction}
\noindent A requirement for a biometric system is being able to to differentiate subjects. Existing image based systems typically use facial recognition \cite{biometric_systems}, whereas video systems may aim to categorise people's gait \cite{Nixon_Tan_Chellappa_2006}. This report identifies the feasibility of body-shape features on a small image dataset with side-on and front-on views of each subject. A second dataset with some subjects repeated is then then used to assess classification performance. Datasets are subsets of the Large Southampton Gait Database \cite{shutler:gait_database} (Figure \ref{fig:dataset_examples}).

\begin{figure}[H]
  \centering
  \begin{tabular}{cc}
  \subfloat[Front]{
  \begin{tabular}{c}
    \includegraphics[width=0.22\textwidth]{figures/024z011pf.jpg}\\
    \includegraphics[width=0.22\textwidth]{figures/DSC00185.jpg}
  \end{tabular}
  }
  \hspace{-6mm}
  \subfloat[Side]{
  \begin{tabular}{c}
    \includegraphics[width=0.22\textwidth]{figures/024z011ps.jpg}\\
    \includegraphics[width=0.22\textwidth]{figures/DSC00186.jpg}
  \end{tabular}
  }
  \end{tabular}
  \caption{Example of subject present in both datasets.}
  \label{fig:dataset_examples}
\end{figure}

\section{Method}
\noindent This section details feature-vector generation and classification methods. In addition to those mentioned, the Python implementation utilised the common libraries: NumPy\footnote{Numpy, https://numpy.org/}, OpenCV\footnote{OpenCV, https://opencv.org/} , PyTorch\footnote{PyTorch, https://pytorch.org/}, and SKLearn\footnote{scikit learn, https://scikit-learn.org/}.


\subsection{Features}
\noindent The final feature-vectors are made from a selection of measurements and moments; those used depend on the view (front/side). Input images were resized to have a height of 600px whilst keeping the same aspect ratio to reduce computation.\\

\subsubsection{Subject Mask}
The first stage of feature extraction was to find which pixels of the input belong to the subject; this is referred to as image segmentation. The subject mask was generated in two stages: 
\begin{enumerate}
  \item Semantic segmentation performed by a Convolutional Neural Network (CNN), specifically DeepLabV3 ResNet101 \cite{DBLP:journals/corr/ChenPSA17}. Torchvision's\footnote{Torchvision, https://pytorch.org/docs/stable/torchvision} implementation was used.
  \item This did not follow the body contours completely (Figure \ref{fig:sem_seg}), so a further green-screen removal stage was done in the HSV colour space. (Figure \ref{fig:final_seg})
\end{enumerate}


\begin{figure}[H]
  \centering
  \begin{tabular}{cccc}
  \subfloat[]{\includegraphics[width=0.10\textwidth]{figures/mask_input.png}}
  \hspace{1.5mm}
  \subfloat[\label{fig:sem_seg}]{\includegraphics[width=0.10\textwidth]{figures/mask_semantic.png}}
  \hspace{1.5mm}
  \subfloat[\label{fig:final_seg}]{\includegraphics[width=0.10\textwidth]{figures/mask_final.png}}
  \hspace{1.5mm}
  \subfloat[]{\includegraphics[width=0.10\textwidth]{figures/mask_contour.png}}
  \end{tabular}
  \caption{Masking example: input, semantically segmented, green screen removed, silhouette with extracted contour points (left--right). Note that the example images are cropped, the actual system accepts full images.}
\end{figure}


\noindent This method lead to perfect mask generation for all subjects. Traditional methods including active contours typically failed where there was crossover with the treadmill.\\

\noindent \textit{Hu Moments} were generated from the full-body mask to form part of the feature vector for the side-on views. These are preferable to area/perimeter as they are invariant to translation, scale and rotation. A \textit{height} metric was also calculated by finding the difference between the lower and upper bounds of the mask. \\

\subsubsection{Keypoint Estimation}
 Fitting a virtual skeleton to a body is known as pose estimation. This method identifies body keypoints with understanding of the underlying semantics that dictate their position; this allows extraction where limbs are occluded or overlap. Torchvision's implementation of Keypoint R-CNN \cite{he2017mask} was used to extract 17 XY keypoints. These were reordered and an extra keypoint was extrapolated for the chest to match OpenPose's\footnote{OpenPose, https://git.io/JfVsu} data format (Figure \ref{fig:poses}).

\begin{figure}[H]
  \centering
  \begin{tabular}{cc}
  \subfloat[]{\includegraphics[height=2.3in]{figures/front_kp_a.png} \includegraphics[height=2.3in]{figures/side_kp_a.png}}
  \hspace{1.5mm}
  \subfloat[]{\includegraphics[height=2.3in]{figures/front_kp_b.png} \includegraphics[height=2.3in]{figures/side_kp_b.png}}
  \end{tabular}
  \caption{Keypoint estimation on two subjects.}\label{fig:poses}
\end{figure}

\noindent These keypoints were accurate where positioning was not subjective, e.g. for noses, but not always where many positions would be acceptable e.g. the pelvis. Additionally, because Z data is not estimated, distances between points cannot take into account depth. Points were therefore mostly used to orientate other features. However, where distances were relatively consistent it was possible to add them to the feature vector; those used are labelled in Figure \ref{fig:measurements}. It would be preferable if these measurements were scale invariant in case the subject stood a different distance from the camera, unfortunately, normalising off one measurement made the features highly non-differentiable. The \textit{view} (\textit{direction}) was determined using the width between shoulder points and the relative position of the nose.

\begin{figure}[H]
  \centering
  \begin{tabular}{cc}
  \subfloat[]{\includegraphics[height=2.3in]{figures/front_measurements_a.png} \includegraphics[height=2.3in]{figures/side_measurements_a.png}}
  \hspace{1.5mm}
  \subfloat[]{\includegraphics[height=2.3in]{figures/front_measurements_b.png} \includegraphics[height=2.3in]{figures/side_measurements_b.png}}
  \end{tabular}
  \caption{Keypoint measurements for each view. red, pink, blue lines indicate where Euclidain distance, Y Manhatten distance and angle (from the horizontal) are used respectively.}\label{fig:measurements}
\end{figure}



\subsubsection{Head Mask Extraction}
Some subjects have different clothes between the validation and testsets; this reduces effectiveness of the full-body mask. Extracting the head portion provides a more consistent shape.

\begin{figure}[H]
  \centering
  \begin{tabular}{cc}
  \subfloat[]{\includegraphics[height=0.75in]{figures/front_head_a.png} \includegraphics[height=0.75in]{figures/side_head_a.png}}
  \hspace{1.5mm}&
  \subfloat[]{\includegraphics[height=0.75in]{figures/front_head_b.png} \includegraphics[height=0.75in]{figures/side_head_b.png}}\\
  \vspace{1.5mm}
  \subfloat[]{\includegraphics[height=0.75in]{figures/front_head_c.png} \includegraphics[height=0.75in]{figures/side_head_c.png}}
  \hspace{1.5mm}&
  \subfloat[]{\includegraphics[height=0.75in]{figures/front_head_d.png} \includegraphics[height=0.75in]{figures/side_head_d.png}}
  \end{tabular}
  \caption{Example head masks (front--side for each subject).}
\end{figure}

\noindent There are no keypoints to make neck extraction straightforward. Simple methods may attempt to find the thinnest point of the mask however this fails with hair and does not take into account the angle of the neck for the side-on view. The following method was used:
\begin{enumerate}
  \item Extract the contour of the full-body.
  \item Trace from the shoulder keypoints to the edge of the contour on the left/right (side-on) and top (front-on) to find two starting points.
  \item Plot the X positions following the contour upwards.
  \item Use peak/trough detection to find the turning points (where the head begins).
  \item Remove proceeding points of the contour to create a contour of just the head.
  \item Convert this contour to a mask.
\end{enumerate}
\textit{Hu Moments} are generated for head masks for all views.


\subsection{Classifier}
\noindent The classifier expects both front-on and side-on views of a subject where the view is inferred as part of feature extraction. The different views use different feature vectors so cannot be directly compared. To this end separate k-nearest-neighbour (k-nn) classifiers are used for each view -- $k=1$ due to lack of repetition in the dataset. These are trained on the \textit{authorised} user set with feature normalisation applied to keep the subject distances between 0 and \mytilde 1. These are combined by taking the mean of the all distance vectors generated; this allows classification via further views or repetitions of the same view. A threshold calculated via the EER is applied to limit false classifications. SVMs were considered, however as their advantages are curtailed by the small dataset, and because they reduce the relevance of the easily comparable distance metric, k-nn was preferable.

\section{Results}
\noindent To assess the system the \texttt{test} dataset (11 classes) were treated as the \textit{authorised} subjects, whereas the \texttt{train} dataset (44 classes) were treated as the \textit{input} subjects. The resultant feature vector distances and top matches are shown in Figure \ref{fig:feature_distances} and Figure \ref{fig:rank_plot} respectively.

\begin{figure}[H]
  \centering
  \begin{tabular}{c}
  \subfloat[Front-On]{\inputpgf{figures/}{front_distances.pgf}}\\
  \subfloat[Side-On]{\inputpgf{figures/}{side_distances.pgf}}\\
  \subfloat[Combined]{\inputpgf{figures/}{combined_distances.pgf}}
  \end{tabular}
  \caption{Log Euclidean distances between subjects. Each row represents an \textit{authorised} subject against all \textit{inputs}. See Figure \ref{fig:ccr_plot} for correct classifications. Temperature indicates closeness (Yellow [Close], Purple [Far]). }\label{fig:feature_distances}
\end{figure}

\vspace{-10mm}
\begin{figure}[H]\centering
  \inputpgf{figures/}{combined_ranked.pgf}
  \caption{Top 3 matches (1--3 [Yellow, Orange, Purple]) for each \textit{authorised} class using the combined metric.}\label{fig:rank_plot}
\end{figure}

\subsection{Accuracy}
\noindent The side-on and front-on classifiers can both be tuned to achieve 63\% CCR individually, however a focus is placed on assessment of the combined system. Metrics were calculated as:
\begin{itemize}
  \item \textbf{CCR:} Correct Recognition Rate -- Percentage of \textit{input} subjects who were closest to their corresponding \textit{authorised} subject.
  \item \textbf{T-CCR:} Threshold-CCR -- CCR but taking into account threshold calculated via EER.
  \item \textbf{FAR:} False Accept Rate -- Percentage of \textit{input} subjects not in the \textit{authorised} dataset who were within the allowed threshold of an \textit{authorised} subject.
  \item \textbf{FRR:} False Reject Rate -- Percentage of \textit{input} subjects who were not within the threshold of their corresponding \textit{authorised} subject.
\end{itemize}

\noindent The EER of 33\% was calculated from the intersect of the FAR and FRR (Figure \ref{fig:eer_plot}), yielding a threshold distance of 0.15. Figure \ref{fig:ccr_plot} shows the accuracy of the system is 91\% (10/11), reduced to 64\% when thresholding (7/11).

\begin{figure}[H]\centering
  \inputpgf{figures/}{eer_plot.pgf}\vspace{-3.5mm}
  \caption{EER Plot -- Intercept = (0.15, 0.33)}\label{fig:eer_plot}
\end{figure}
\vspace{-5.0mm}
\begin{figure}[H]\centering
  \inputpgf{figures/}{ccr_plot.pgf}
  \caption{T-CCR Plot -- True positives, false negatives and false positives shown by green, orange, red respectively. Note where classified incorrectly, green identifies the correct class.}\label{fig:ccr_plot}
\end{figure}

\subsection{Speed}
\noindent A single recognition took 10.2s running on a CPU (Intel i7 8700K). Much of this time was taken up by the segmentation and keypoint detection performed by neural networks. However, these were integrated with GPU support so when tied with a Nvidia GeForce 1080 Ti this time was 0.6s.

\section{Discussion}
\subsection{Performance}
\noindent The results are promising, suggesting that subjects can be differentiated based on their body shape. However, performance is not comparable to facial recognition systems, with modern implementations achieving EERs of 0.08\% and older systems still achieving 4.1\% \cite{facial_rec}. The comparably high EER can largely be attributed to the inter-class variance of the features not being high as shown by Figure \ref{fig:feature_distances}; this implies low confidence in predictions. The results are mostly influenced by the head Hu Moments and height measurement, with which a CCR of  64\% (7/11) could be achieved. The incorporation of other measurements lead to the CCR of 91\%, however it could be deemed that these additions lead to overfitting. Likewise, it is common for a single extracted feature to match badly; this increases the overall distance which leads to the EER threshold having to be increased. This is however hard to assess due to lack of different validation / test sets. The often negative effect of the full-body Hu Moments highlight the issues of recognising by body shape for clothed subjects. Avoiding usage of clothed areas is a challenge in itself and reduces the number of usable features. The overall running time suggests that real-time performance is viable for stills but not video, however, it should be noted that the system was not optimised. \\

\subsection{Improvements}
\noindent There are many features that could be implemented in the future including but not limited to: edge analysis via Gabor wavelets or training a CNN to find border patterns. More importantly, effort should be placed on making the system fully invariant to subject-to-camera distance and to reduce the effect of limb occlusion/depth. This could be achieved by combining a multi-camera setup that can handle 3D pose estimation with body measurement normalisation (i.e. body ratios).\\

\noindent Classification could also be more adaptable. The side-classifier typically had a higher degree of confidence than the front-classifier. Performance could likely be improved by weighting the components of the combined classifier based on confidence level. This would be especially useful if other views could be added.

\noindent Speed-ups can be achieved by implementing a single CNN for both segmentation and pose estimation; reducing computation by half. Furthermore additional caching could be implemented to stop features being generated multiple times for a single feature-vector.


\section{Conclusion}
\noindent A system has been proposed that is able to classify individuals purely on body-shape; this reduces dependency on a subject's face or video source of movement. The system's low-confidence (similarities in distances) highlight the issues with recognising clothed subjects and indicate further views or averaging across a larger dataset would improve performance substantially. Other improvements have been detailed in the discussion.

\bibliographystyle{IEEEtranUrldate}
\bibliography{mybibfile}

%\begin{thebibliography}{00}
%\bibitem{b1} Bluetooth SIG, \textit{``Bluetooth Market Update
%2019''}, White Paper, 2018
%\bibitem{b2} Bluetooth SIG, \textit{``Bluetooth Core Specification
%v5.1''}, White Paper, Bluetooth SIG, 21st January 2019
%\end{thebibliography}
\end{document}